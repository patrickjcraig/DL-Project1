{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from models.CNN import create_cnn_model\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    # Move up one directory when loading the data\n",
    "    file_path = os.path.join('../../../', f)\n",
    "    return np.load(file_path)['arr_0']\n",
    "\n",
    "# Load the data\n",
    "X_train = load('kmnist-train-imgs.npz')/255.0\n",
    "x_test = load('kmnist-test-imgs.npz')/255.0\n",
    "Y_train = load('kmnist-train-labels.npz')\n",
    "y_test = load('kmnist-test-labels.npz')\n",
    "# Reshape the data for CNN input\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape and number of classes\n",
    "input_shape = X_train.shape[1:]  # 784 for KMNIST\n",
    "num_classes = Y_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: {'batch_normalization': False, 'pooling': 'max'}\n",
      "Epoch 1/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4720 - accuracy: 0.8490 - val_loss: 0.2665 - val_accuracy: 0.9190\n",
      "Epoch 2/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1775 - accuracy: 0.9446 - val_loss: 0.1600 - val_accuracy: 0.9487\n",
      "Epoch 3/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1211 - accuracy: 0.9625 - val_loss: 0.1345 - val_accuracy: 0.9548\n",
      "Epoch 4/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0912 - accuracy: 0.9716 - val_loss: 0.1098 - val_accuracy: 0.9640\n",
      "Epoch 5/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0718 - accuracy: 0.9772 - val_loss: 0.1086 - val_accuracy: 0.9653\n",
      "Epoch 6/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0550 - accuracy: 0.9820 - val_loss: 0.1061 - val_accuracy: 0.9682\n",
      "Epoch 7/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0466 - accuracy: 0.9845 - val_loss: 0.0994 - val_accuracy: 0.9687\n",
      "Epoch 8/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0365 - accuracy: 0.9878 - val_loss: 0.0914 - val_accuracy: 0.9742\n",
      "Epoch 9/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.9903 - val_loss: 0.1214 - val_accuracy: 0.9680\n",
      "Epoch 10/20\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0270 - accuracy: 0.9906 - val_loss: 0.1207 - val_accuracy: 0.9690\n",
      "Epoch 11/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0235 - accuracy: 0.9921 - val_loss: 0.1119 - val_accuracy: 0.9720\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Testing configuration: {'batch_normalization': True, 'pooling': 'max'}\n",
      "Epoch 1/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.2114 - accuracy: 0.9427 - val_loss: 0.1088 - val_accuracy: 0.9647\n",
      "Epoch 2/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0666 - accuracy: 0.9806 - val_loss: 0.0741 - val_accuracy: 0.9760\n",
      "Epoch 3/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0442 - accuracy: 0.9871 - val_loss: 0.0654 - val_accuracy: 0.9817\n",
      "Epoch 4/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0326 - accuracy: 0.9900 - val_loss: 0.1068 - val_accuracy: 0.9673\n",
      "Epoch 5/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.0959 - val_accuracy: 0.9707\n",
      "Epoch 6/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0205 - accuracy: 0.9936 - val_loss: 0.0579 - val_accuracy: 0.9830\n",
      "Epoch 7/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.0633 - val_accuracy: 0.9832\n",
      "Epoch 8/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0374 - val_accuracy: 0.9890\n",
      "Epoch 9/20\n",
      "1688/1688 [==============================] - 9s 6ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.0507 - val_accuracy: 0.9852\n",
      "Epoch 10/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0794 - val_accuracy: 0.9782\n",
      "Epoch 11/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0442 - val_accuracy: 0.9878\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "Testing configuration: {'batch_normalization': False, 'pooling': 'avg'}\n",
      "Epoch 1/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.6542 - accuracy: 0.7847 - val_loss: 0.3321 - val_accuracy: 0.8920\n",
      "Epoch 2/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2695 - accuracy: 0.9149 - val_loss: 0.2047 - val_accuracy: 0.9355\n",
      "Epoch 3/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1872 - accuracy: 0.9405 - val_loss: 0.1597 - val_accuracy: 0.9510\n",
      "Epoch 4/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1454 - accuracy: 0.9540 - val_loss: 0.1529 - val_accuracy: 0.9512\n",
      "Epoch 5/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1213 - accuracy: 0.9620 - val_loss: 0.1280 - val_accuracy: 0.9600\n",
      "Epoch 6/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1034 - accuracy: 0.9671 - val_loss: 0.1133 - val_accuracy: 0.9650\n",
      "Epoch 7/20\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0880 - accuracy: 0.9724 - val_loss: 0.1141 - val_accuracy: 0.9635\n",
      "Epoch 8/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0758 - accuracy: 0.9755 - val_loss: 0.1198 - val_accuracy: 0.9622\n",
      "Epoch 9/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0656 - accuracy: 0.9785 - val_loss: 0.1044 - val_accuracy: 0.9677\n",
      "Epoch 10/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0557 - accuracy: 0.9820 - val_loss: 0.0965 - val_accuracy: 0.9697\n",
      "Epoch 11/20\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.0486 - accuracy: 0.9845 - val_loss: 0.0977 - val_accuracy: 0.9720\n",
      "Epoch 12/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0429 - accuracy: 0.9858 - val_loss: 0.0941 - val_accuracy: 0.9722\n",
      "Epoch 13/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0361 - accuracy: 0.9883 - val_loss: 0.0955 - val_accuracy: 0.9750\n",
      "Epoch 14/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0321 - accuracy: 0.9892 - val_loss: 0.1099 - val_accuracy: 0.9708\n",
      "Epoch 15/20\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 0.0990 - val_accuracy: 0.9725\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Testing configuration: {'batch_normalization': True, 'pooling': 'avg'}\n",
      "Epoch 1/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1909 - accuracy: 0.9505 - val_loss: 0.0737 - val_accuracy: 0.9782\n",
      "Epoch 2/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0659 - accuracy: 0.9810 - val_loss: 0.0681 - val_accuracy: 0.9758\n",
      "Epoch 3/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0457 - accuracy: 0.9863 - val_loss: 0.0554 - val_accuracy: 0.9833\n",
      "Epoch 4/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0361 - accuracy: 0.9886 - val_loss: 0.0515 - val_accuracy: 0.9840\n",
      "Epoch 5/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0573 - val_accuracy: 0.9823\n",
      "Epoch 6/20\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.0431 - val_accuracy: 0.9863\n",
      "Epoch 7/20\n",
      "1688/1688 [==============================] - 9s 6ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.0397 - val_accuracy: 0.9897\n",
      "Epoch 8/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.0394 - val_accuracy: 0.9885\n",
      "Epoch 9/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0312 - val_accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.0536 - val_accuracy: 0.9865\n",
      "Epoch 11/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.0429 - val_accuracy: 0.9885\n",
      "Epoch 12/20\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.0415 - val_accuracy: 0.9912\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "Best configuration: {'batch_normalization': True, 'pooling': 'avg'}\n",
      "Best accuracy: 0.9911666512489319\n",
      "Training best model on full training set...\n",
      "Epoch 1/100\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.2058 - accuracy: 0.9424 - val_loss: 0.1165 - val_accuracy: 0.9615\n",
      "Epoch 2/100\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0807 - accuracy: 0.9763 - val_loss: 0.0580 - val_accuracy: 0.9833\n",
      "Epoch 3/100\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.0514 - val_accuracy: 0.9840\n",
      "Epoch 4/100\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0457 - accuracy: 0.9859 - val_loss: 0.0484 - val_accuracy: 0.9857\n",
      "Epoch 5/100\n",
      "3375/3375 [==============================] - 23s 7ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 0.0329 - val_accuracy: 0.9900\n",
      "Epoch 6/100\n",
      "3375/3375 [==============================] - 28s 8ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0375 - val_accuracy: 0.9905\n",
      "Epoch 7/100\n",
      "3375/3375 [==============================] - 32s 9ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.0461 - val_accuracy: 0.9868\n",
      "Epoch 8/100\n",
      "3375/3375 [==============================] - 31s 9ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0393 - val_accuracy: 0.9898\n",
      "Epoch 9/100\n",
      "3375/3375 [==============================] - 29s 8ms/step - loss: 0.0213 - accuracy: 0.9934 - val_loss: 0.0316 - val_accuracy: 0.9903\n",
      "Epoch 10/100\n",
      "3375/3375 [==============================] - 33s 10ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.0324 - val_accuracy: 0.9910\n",
      "Epoch 11/100\n",
      "3375/3375 [==============================] - 28s 8ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.0515 - val_accuracy: 0.9860\n",
      "Epoch 12/100\n",
      "3375/3375 [==============================] - 33s 10ms/step - loss: 0.0138 - accuracy: 0.9956 - val_loss: 0.0328 - val_accuracy: 0.9927\n",
      "Epoch 13/100\n",
      "3375/3375 [==============================] - 28s 8ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.0300 - val_accuracy: 0.9923\n",
      "Epoch 14/100\n",
      "3375/3375 [==============================] - 31s 9ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0327 - val_accuracy: 0.9918\n",
      "Epoch 15/100\n",
      "3375/3375 [==============================] - 30s 9ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0369 - val_accuracy: 0.9910\n",
      "Epoch 16/100\n",
      "3375/3375 [==============================] - 34s 10ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.0382 - val_accuracy: 0.9897\n",
      "Epoch 17/100\n",
      "3375/3375 [==============================] - 30s 9ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0257 - val_accuracy: 0.9918\n",
      "Epoch 18/100\n",
      "3375/3375 [==============================] - 31s 9ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0324 - val_accuracy: 0.9917\n",
      "Epoch 19/100\n",
      "3375/3375 [==============================] - 29s 8ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0377 - val_accuracy: 0.9908\n",
      "Epoch 20/100\n",
      "3375/3375 [==============================] - 24s 7ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0433 - val_accuracy: 0.9893\n",
      "Epoch 21/100\n",
      "3375/3375 [==============================] - 24s 7ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0344 - val_accuracy: 0.9907\n",
      "Epoch 22/100\n",
      "3375/3375 [==============================] - 23s 7ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0297 - val_accuracy: 0.9938\n",
      "Epoch 23/100\n",
      "3375/3375 [==============================] - 23s 7ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0340 - val_accuracy: 0.9925\n",
      "Epoch 24/100\n",
      "3375/3375 [==============================] - 23s 7ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0327 - val_accuracy: 0.9913\n",
      "Epoch 25/100\n",
      "3375/3375 [==============================] - 25s 7ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0315 - val_accuracy: 0.9923\n",
      "Epoch 26/100\n",
      "3375/3375 [==============================] - 22s 6ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0386 - val_accuracy: 0.9923\n",
      "Epoch 27/100\n",
      "3375/3375 [==============================] - 22s 6ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.9928\n",
      "Test accuracy: 0.9721\n",
      "Test loss: 0.1304\n",
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'num_layers': 3,\n",
    "    'filters': [32, 64, 64],\n",
    "    'kernel_sizes': [(3, 3), (3, 3), (3, 3)],\n",
    "    'activations': ['relu', 'relu', 'relu'],\n",
    "    'dense_units': 64,\n",
    "    'dense_activation': 'relu'\n",
    "}\n",
    "configs = [\n",
    "    {'batch_normalization': False, 'pooling': 'max'},\n",
    "    {'batch_normalization': True, 'pooling': 'max'},\n",
    "    {'batch_normalization': False, 'pooling': 'avg'},\n",
    "    {'batch_normalization': True, 'pooling': 'avg'}\n",
    "]\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "best_config = None\n",
    "best_accuracy = 0\n",
    "histories = []\n",
    "\n",
    "test_loss_df = pd.DataFrame()\n",
    "test_accuracy_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"Testing configuration: {config}\")\n",
    "    hyperparameters.update(config)\n",
    "    model = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=20,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping])\n",
    "    # Learning curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Learning Curve for Config: {config[\"pooling\"]} {config[\"batch_normalization\"]}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    now = datetime.datetime.now()\n",
    "    plt.savefig(f'learning_curve_{config[\"pooling\"]}_{config[\"batch_normalization\"]}_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "    plt.close()\n",
    "    # Append the last value of val_loss and val_accuracy an additional ten times\n",
    "    last_val_loss = history.history['val_loss'][-1]\n",
    "    last_val_accuracy = history.history['val_accuracy'][-1]\n",
    "    for _ in range(len(history.history['val_loss']), 100):\n",
    "        history.history['val_loss'].append(last_val_loss)\n",
    "        history.history['val_accuracy'].append(last_val_accuracy)\n",
    "    test_loss_df[f\"{config['pooling']} {config['batch_normalization']}\"] = history.history['val_loss']\n",
    "    test_accuracy_df[f\"{config['pooling']} {config['batch_normalization']}\"] = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "    # Predictions on the validation set\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    # Confusion matrix\n",
    "    confusion = confusion_matrix(y_test, y_pred_class)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
    "    disp.plot()\n",
    "\n",
    "    plt.title(f'Confusion Matrix for Config: {config[\"pooling\"]} {config[\"batch_normalization\"]}')\n",
    "    plt.savefig(f'confusion_matrix_{config[\"pooling\"]}_{config[\"batch_normalization\"]}_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "    plt.close()\n",
    "    histories.append(history)\n",
    "    final_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "    if final_acc > best_accuracy:\n",
    "        best_accuracy = final_acc\n",
    "        best_config = config\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "# Train the best model on the full training set and evaluate on test set\n",
    "print(\"Training best model on full training set...\")\n",
    "hyperparameters = hyperparameters.copy()\n",
    "hyperparameters.update(best_config)\n",
    "\n",
    "# Create and compile the best model\n",
    "best_model = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# Train on the full training set\n",
    "history = best_model.fit(x_train, y_train, \n",
    "                         epochs=100, \n",
    "                         batch_size=16, \n",
    "                         validation_data=(x_val, y_val),\n",
    "                         callbacks=[early_stopping],\n",
    "                         verbose=1)\n",
    "# Append the last value of val_loss and val_accuracy an additional # of times so it can be plotted on same graph as other models\n",
    "last_val_loss = history.history['val_loss'][-1]\n",
    "last_val_accuracy = history.history['val_accuracy'][-1]\n",
    "for _ in range(len(history.history['val_loss']), 100):\n",
    "    history.history['val_loss'].append(last_val_loss)\n",
    "    history.history['val_accuracy'].append(last_val_accuracy)\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Plot the learnin curve of the best model\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(f'Learning Curve for Best Model: {best_config[\"pooling\"]} {best_config[\"batch_normalization\"]}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "now = datetime.datetime.now()\n",
    "plt.savefig(f'learning_curve_best_{best_config[\"pooling\"]}_{best_config[\"batch_normalization\"]}_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "plt.close()\n",
    "# Predictions on the test set\n",
    "y_pred = best_model.predict(x_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
    "disp.plot()\n",
    "plt.title(f'Confusion Matrix for Best Model: {best_config[\"pooling\"]} {best_config[\"batch_normalization\"]}')\n",
    "plt.savefig(f'confusion_matrix_best_{best_config[\"pooling\"]}_{best_config[\"batch_normalization\"]}_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "plt.close()\n",
    "# Add the \"best + config\" column\n",
    "test_loss_df[f\"best {best_config['pooling']} {best_config['batch_normalization']}\"] = history.history['val_loss']\n",
    "test_accuracy_df[f\"best {best_config['pooling']} {best_config['batch_normalization']}\"] = history.history['val_accuracy']\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "test_loss_df.to_csv('test_loss.csv', index=False)\n",
    "test_accuracy_df.to_csv('test_accuracy.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9911666512489319\n"
     ]
    }
   ],
   "source": [
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameters.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(image, mask_size=16, num_cutouts=1):\n",
    "    \"\"\"Applies Cutout augmentation to an image.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    image_copy = image.copy()\n",
    "\n",
    "    for _ in range(num_cutouts):\n",
    "        x = np.random.randint(0, w)\n",
    "        y = np.random.randint(0, h)\n",
    "\n",
    "        # Ensure the cutout does not go out of bounds\n",
    "        x1 = np.clip(x - mask_size // 2, 0, w)\n",
    "        x2 = np.clip(x + mask_size // 2, 0, w)\n",
    "        y1 = np.clip(y - mask_size // 2, 0, h)\n",
    "        y2 = np.clip(y + mask_size // 2, 0, h)\n",
    "\n",
    "        # Set the cutout region to the mean value of the image\n",
    "        mask_color = np.mean(image_copy)\n",
    "        image_copy[y1:y2, x1:x2, :] = mask_color\n",
    "\n",
    "    return image_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random noise\n",
    "def add_random_noise(image, noise_factor=0.1):\n",
    "    \"\"\"Adds random noise to an image.\"\"\"\n",
    "    noisy_image = image + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=image.shape)\n",
    "    return np.clip(noisy_image, 0.0, 1.0)\n",
    "\n",
    "def add_radial_noise(image, noise_factor=0.1):\n",
    "    \"\"\"Adds radial noise to an image.\"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    max_dist = np.sqrt(center_x**2 + center_y**2)\n",
    "    normalized_dist = dist_from_center / max_dist\n",
    "    noise = np.random.normal(loc=0.0, scale=1.0, size=image.shape)\n",
    "    radial_noise = noise * normalized_dist[:, :, np.newaxis] * noise_factor\n",
    "    noisy_image = image + radial_noise\n",
    "    return np.clip(noisy_image, 0.0, 1.0)\n",
    "\n",
    "def add_mask(image, mask_size=16, num_masks=1):\n",
    "    \"\"\"Adds random masks to an image.\"\"\"\n",
    "    masked_image = image.copy()\n",
    "    h, w, _ = image.shape\n",
    "    for _ in range(num_masks):\n",
    "        y = np.random.randint(0, h - mask_size)\n",
    "        x = np.random.randint(0, w - mask_size)\n",
    "        masked_image[y:y+mask_size, x:x+mask_size, :] = 0\n",
    "    return masked_image\n",
    "\n",
    "def add_random_noise_mask(image):\n",
    "    altered_image = add_random_noise(image)\n",
    "    altered_image = add_mask(altered_image)\n",
    "    return altered_image\n",
    "\n",
    "datagen_rand_noise = ImageDataGenerator(\n",
    "    preprocessing_function=add_random_noise  # Noise injection\n",
    ")\n",
    "\n",
    "# Radial noise\n",
    "datagen_radial_noise = ImageDataGenerator(\n",
    "    preprocessing_function=add_radial_noise  # Radial noise injection\n",
    ")\n",
    "\n",
    "# Flip\n",
    "datagen_flip = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "# Zoom\n",
    "datagen_zoom = ImageDataGenerator(\n",
    "    zoom_range=0.5 # Zoom in on the image\n",
    ")\n",
    "\n",
    "# Mask\n",
    "datagen_mask = ImageDataGenerator(\n",
    "    preprocessing_function = cutout  # Zoom in on the image\n",
    ")\n",
    "\n",
    "# Random noise and zoom\n",
    "datagen_rand_noise_zoom = ImageDataGenerator(\n",
    "    preprocessing_function=add_random_noise,  # Noise injection\n",
    "    zoom_range=0.1 # Zoom in on the image\n",
    ")\n",
    "\n",
    "# Random noise and mask\n",
    "datagen_rand_noise_mask = ImageDataGenerator(\n",
    "    preprocessing_function=add_random_noise_mask,  # Noise injection\n",
    ")\n",
    "\n",
    "# No noise\n",
    "datagen_no_noise = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Noise + Mask')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a sample image\n",
    "sample_image = x_train[0]\n",
    "img_arr = img_to_array(sample_image) \n",
    "img_arr = img_arr.reshape((1,) + img_arr.shape)\n",
    "# Apply different augmentations\n",
    "augmented_images = []\n",
    "# For the other augmentations, we can use the existing datagenerators\n",
    "\n",
    "# Apply each datagen separately to the same image\n",
    "augmented_images.append(img_arr)  # Original image\n",
    "\n",
    "# Random Noise\n",
    "augmented_images.append(datagen_rand_noise.flow(img_arr, batch_size=1))\n",
    "\n",
    "# Radial Noise\n",
    "augmented_images.append(datagen_radial_noise.flow(img_arr, batch_size=1))\n",
    "\n",
    "# Flip\n",
    "augmented_images.append(datagen_flip.flow(img_arr, batch_size=1))\n",
    "\n",
    "# Zoom\n",
    "augmented_images.append(datagen_zoom.flow(img_arr, batch_size=1))\n",
    "\n",
    "# Mask (Cutout)\n",
    "augmented_images.append(datagen_mask.flow(img_arr, batch_size=1))\n",
    "\n",
    "# Random Noise + Zoom\n",
    "augmented_images.append(datagen_rand_noise_zoom.flow(img_arr, batch_size=1))\n",
    "\n",
    "# No Augmentation\n",
    "augmented_images.append(datagen_rand_noise_mask.flow(img_arr, batch_size=1))\n",
    "\n",
    "f, xyarr = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for ax in xyarr.flat:\n",
    "    ax.axis('off')  # This removes all axes, including scale bars\n",
    "plt.rcParams.update({'font.size': 12})  # Increase font size\n",
    "xyarr[0,0].imshow(augmented_images[0][0].squeeze(), cmap='gray')\n",
    "xyarr[0,0].set_title('Original')\n",
    "xyarr[0,1].imshow(augmented_images[1][0].squeeze(), cmap='gray')\n",
    "xyarr[0,1].set_title('Random Noise')\n",
    "xyarr[0,2].imshow(augmented_images[2][0].squeeze(), cmap='gray')\n",
    "xyarr[0,2].set_title('Radial Noise')\n",
    "xyarr[0,3].imshow(augmented_images[3][0].squeeze(), cmap='gray')\n",
    "xyarr[0,3].set_title('Flip')    \n",
    "xyarr[1,0].imshow(augmented_images[4][0].squeeze(), cmap='gray')\n",
    "xyarr[1,0].set_title('Zoom')\n",
    "xyarr[1,1].imshow(augmented_images[5][0].squeeze(), cmap='gray')\n",
    "xyarr[1,1].set_title('Cutout')\n",
    "xyarr[1,2].imshow(augmented_images[6][0].squeeze(), cmap='gray')\n",
    "xyarr[1,2].set_title('Noise + Zoom')\n",
    "xyarr[1,3].imshow(augmented_images[7][0].squeeze(), cmap='gray')\n",
    "xyarr[1,3].set_title('Noise + Mask')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 3, 'filters': [32, 64, 64], 'kernel_sizes': [[3, 3], [3, 3], [3, 3]], 'activations': ['relu', 'relu', 'relu'], 'dense_units': 64, 'dense_activation': 'relu', 'batch_normalization': True, 'pooling': 'avg'}\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1970 - accuracy: 0.9482 - val_loss: 0.1445 - val_accuracy: 0.9525\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0730 - accuracy: 0.9787 - val_loss: 0.0934 - val_accuracy: 0.9720\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0527 - accuracy: 0.9840 - val_loss: 0.0658 - val_accuracy: 0.9813\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0392 - accuracy: 0.9882 - val_loss: 0.0511 - val_accuracy: 0.9847\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 0.0485 - val_accuracy: 0.9847\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 0.0498 - val_accuracy: 0.9847\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0526 - val_accuracy: 0.9842\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.0373 - val_accuracy: 0.9898\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0398 - val_accuracy: 0.9885\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0300 - val_accuracy: 0.9920\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0389 - val_accuracy: 0.9882\n",
      "Epoch 12/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0410 - val_accuracy: 0.9892\n",
      "Epoch 13/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0567 - val_accuracy: 0.9860\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1971 - accuracy: 0.9471 - val_loss: 0.1016 - val_accuracy: 0.9695\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0668 - accuracy: 0.9801 - val_loss: 0.0964 - val_accuracy: 0.9665\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0451 - accuracy: 0.9865 - val_loss: 0.0427 - val_accuracy: 0.9868\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0356 - accuracy: 0.9893 - val_loss: 0.0646 - val_accuracy: 0.9810\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 0.0480 - val_accuracy: 0.9862\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.0860 - val_accuracy: 0.9728\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.4875 - accuracy: 0.8527 - val_loss: 0.2953 - val_accuracy: 0.9058\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.2185 - accuracy: 0.9332 - val_loss: 0.2329 - val_accuracy: 0.9250\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1678 - accuracy: 0.9482 - val_loss: 0.1779 - val_accuracy: 0.9423\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1406 - accuracy: 0.9558 - val_loss: 0.1428 - val_accuracy: 0.9578\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1240 - accuracy: 0.9617 - val_loss: 0.1156 - val_accuracy: 0.9630\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1103 - accuracy: 0.9655 - val_loss: 0.1403 - val_accuracy: 0.9565\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1019 - accuracy: 0.9677 - val_loss: 0.1121 - val_accuracy: 0.9633\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0945 - accuracy: 0.9705 - val_loss: 0.1033 - val_accuracy: 0.9680\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0831 - accuracy: 0.9739 - val_loss: 0.1058 - val_accuracy: 0.9688\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0815 - accuracy: 0.9749 - val_loss: 0.0971 - val_accuracy: 0.9703\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0749 - accuracy: 0.9766 - val_loss: 0.0957 - val_accuracy: 0.9683\n",
      "Epoch 12/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0712 - accuracy: 0.9779 - val_loss: 0.1032 - val_accuracy: 0.9697\n",
      "Epoch 13/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0671 - accuracy: 0.9783 - val_loss: 0.0811 - val_accuracy: 0.9750\n",
      "Epoch 14/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0643 - accuracy: 0.9797 - val_loss: 0.0763 - val_accuracy: 0.9775\n",
      "Epoch 15/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0621 - accuracy: 0.9807 - val_loss: 0.0786 - val_accuracy: 0.9765\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.3882 - accuracy: 0.8845 - val_loss: 0.1130 - val_accuracy: 0.9658\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1745 - accuracy: 0.9470 - val_loss: 0.0750 - val_accuracy: 0.9767\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1322 - accuracy: 0.9593 - val_loss: 0.3145 - val_accuracy: 0.8968\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1122 - accuracy: 0.9654 - val_loss: 0.0538 - val_accuracy: 0.9848\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1022 - accuracy: 0.9689 - val_loss: 0.0581 - val_accuracy: 0.9823\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0894 - accuracy: 0.9720 - val_loss: 0.0540 - val_accuracy: 0.9847\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.0845 - accuracy: 0.9736 - val_loss: 0.0438 - val_accuracy: 0.9862\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0770 - accuracy: 0.9766 - val_loss: 0.0415 - val_accuracy: 0.9882\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0709 - accuracy: 0.9777 - val_loss: 0.0506 - val_accuracy: 0.9857\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0664 - accuracy: 0.9789 - val_loss: 0.0570 - val_accuracy: 0.9833\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0648 - accuracy: 0.9794 - val_loss: 0.0441 - val_accuracy: 0.9872\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.4129 - accuracy: 0.8734 - val_loss: 0.1409 - val_accuracy: 0.9552\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.2121 - accuracy: 0.9324 - val_loss: 0.0709 - val_accuracy: 0.9760\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1716 - accuracy: 0.9449 - val_loss: 0.0632 - val_accuracy: 0.9805\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1439 - accuracy: 0.9539 - val_loss: 0.0435 - val_accuracy: 0.9872\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1357 - accuracy: 0.9565 - val_loss: 0.0434 - val_accuracy: 0.9855\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1212 - accuracy: 0.9603 - val_loss: 0.0517 - val_accuracy: 0.9838\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.1151 - accuracy: 0.9624 - val_loss: 0.0378 - val_accuracy: 0.9898\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.1046 - accuracy: 0.9660 - val_loss: 0.0408 - val_accuracy: 0.9867\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0998 - accuracy: 0.9674 - val_loss: 0.0383 - val_accuracy: 0.9893\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.1008 - accuracy: 0.9668 - val_loss: 0.0365 - val_accuracy: 0.9900\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0934 - accuracy: 0.9699 - val_loss: 0.0343 - val_accuracy: 0.9895\n",
      "Epoch 12/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0889 - accuracy: 0.9706 - val_loss: 0.0371 - val_accuracy: 0.9897\n",
      "Epoch 13/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0842 - accuracy: 0.9724 - val_loss: 0.0325 - val_accuracy: 0.9913\n",
      "Epoch 14/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0819 - accuracy: 0.9731 - val_loss: 0.0281 - val_accuracy: 0.9922\n",
      "Epoch 15/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.0813 - accuracy: 0.9734 - val_loss: 0.0307 - val_accuracy: 0.9923\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2063 - accuracy: 0.9441 - val_loss: 0.0877 - val_accuracy: 0.9722\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0796 - accuracy: 0.9763 - val_loss: 0.0535 - val_accuracy: 0.9820\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0586 - accuracy: 0.9823 - val_loss: 0.0574 - val_accuracy: 0.9820\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 14s 9ms/step - loss: 0.0477 - accuracy: 0.9849 - val_loss: 0.0472 - val_accuracy: 0.9843\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.0497 - val_accuracy: 0.9845\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0344 - accuracy: 0.9887 - val_loss: 0.0694 - val_accuracy: 0.9792\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0292 - accuracy: 0.9909 - val_loss: 0.0406 - val_accuracy: 0.9888\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0470 - val_accuracy: 0.9873\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0368 - val_accuracy: 0.9897\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.0473 - val_accuracy: 0.9870\n",
      "Epoch 11/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0397 - val_accuracy: 0.9885\n",
      "Epoch 12/15\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0494 - val_accuracy: 0.9883\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.5882 - accuracy: 0.8140 - val_loss: 0.2025 - val_accuracy: 0.9338\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.3471 - accuracy: 0.8865 - val_loss: 0.1390 - val_accuracy: 0.9545\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.2896 - accuracy: 0.9033 - val_loss: 0.1321 - val_accuracy: 0.9595\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.2546 - accuracy: 0.9166 - val_loss: 0.1869 - val_accuracy: 0.9428\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 12s 7ms/step - loss: 0.2443 - accuracy: 0.9196 - val_loss: 0.1042 - val_accuracy: 0.9702\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2241 - accuracy: 0.9263 - val_loss: 0.1004 - val_accuracy: 0.9692\n",
      "Epoch 7/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2115 - accuracy: 0.9300 - val_loss: 0.0724 - val_accuracy: 0.9783\n",
      "Epoch 8/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1996 - accuracy: 0.9348 - val_loss: 0.1087 - val_accuracy: 0.9678\n",
      "Epoch 9/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1956 - accuracy: 0.9353 - val_loss: 0.1283 - val_accuracy: 0.9627\n",
      "Epoch 10/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1851 - accuracy: 0.9394 - val_loss: 0.1120 - val_accuracy: 0.9655\n",
      "Epoch 1/15\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1967 - accuracy: 0.9476 - val_loss: 0.0793 - val_accuracy: 0.9765\n",
      "Epoch 2/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0666 - accuracy: 0.9804 - val_loss: 0.1180 - val_accuracy: 0.9627\n",
      "Epoch 3/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0462 - accuracy: 0.9865 - val_loss: 0.0538 - val_accuracy: 0.9833\n",
      "Epoch 4/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0353 - accuracy: 0.9895 - val_loss: 0.0568 - val_accuracy: 0.9825\n",
      "Epoch 5/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.0680 - val_accuracy: 0.9803\n",
      "Epoch 6/15\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0554 - val_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperparameters\n",
    "hyperparameters = json.load(open('hyperparameters.json'))\n",
    "print(hyperparameters)\n",
    "# Create the models with different noise injection\n",
    "best_model_rand_noise = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_radial_noise = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_flip = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_zoom = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_mask = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_rand_noise_zoom = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_rand_noise_mask = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "best_model_no_noise = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "\n",
    "\n",
    "\n",
    "datagen_val = ImageDataGenerator()\n",
    "# Wrap the datagen with our custom generator\n",
    "train_generator_rand_noise = datagen_rand_noise.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_radial_noise = datagen_radial_noise.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_flip = datagen_flip.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_zoom = datagen_zoom.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_mask = datagen_mask.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_rand_noise_zoom = datagen_rand_noise_zoom.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_rand_noise_mask = datagen_rand_noise_mask.flow(x_train, y_train, batch_size=32)\n",
    "train_generator_no_noise = datagen_no_noise.flow(x_train, y_train, batch_size=32)\n",
    "val_generator = datagen_val.flow(x_val, y_val, batch_size=32)\n",
    "# Train the model with custom augmentation\n",
    "augmentation_loss_df = pd.DataFrame()\n",
    "augmentation_accuracy_df = pd.DataFrame()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# Random noise\n",
    "history_rand_noise = best_model_rand_noise.fit(train_generator_rand_noise,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,  \n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_rand_noise.history['val_loss'][-1]\n",
    "last_val_accuracy = history_rand_noise.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_rand_noise.history['val_loss']), 16):\n",
    "    history_rand_noise.history['val_loss'].append(last_val_loss)\n",
    "    history_rand_noise.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"rand_noise\"] = history_rand_noise.history['val_loss']\n",
    "augmentation_accuracy_df[f\"rand_noise\"] = history_rand_noise.history['val_accuracy']\n",
    "\n",
    "# Radial noise\n",
    "history_radial_noise = best_model_radial_noise.fit(train_generator_radial_noise,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,  \n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_radial_noise.history['val_loss'][-1]\n",
    "last_val_accuracy = history_radial_noise.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_radial_noise.history['val_loss']), 16):\n",
    "    history_radial_noise.history['val_loss'].append(last_val_loss)\n",
    "    history_radial_noise.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"radial_noise\"] = history_radial_noise.history['val_loss']\n",
    "augmentation_accuracy_df[f\"radial_noise\"] = history_radial_noise.history['val_accuracy']\n",
    "\n",
    "\n",
    "# Flip\n",
    "history_flip = best_model_flip.fit(train_generator_flip,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_flip.history['val_loss'][-1]\n",
    "last_val_accuracy = history_flip.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_flip.history['val_loss']), 16):\n",
    "    history_flip.history['val_loss'].append(last_val_loss)\n",
    "    history_flip.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"flip\"] = history_flip.history['val_loss']\n",
    "augmentation_accuracy_df[f\"flip\"] = history_flip.history['val_accuracy']\n",
    "\n",
    "# Zoom\n",
    "history_zoom = best_model_zoom.fit(train_generator_zoom,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_zoom.history['val_loss'][-1]\n",
    "last_val_accuracy = history_zoom.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_zoom.history['val_loss']), 16):\n",
    "    history_zoom.history['val_loss'].append(last_val_loss)\n",
    "    history_zoom.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"zoom\"] = history_zoom.history['val_loss']\n",
    "augmentation_accuracy_df[f\"zoom\"] = history_zoom.history['val_accuracy']\n",
    "\n",
    "# Mask\n",
    "history_mask = best_model_mask.fit(train_generator_mask,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_mask.history['val_loss'][-1]\n",
    "last_val_accuracy = history_mask.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_mask.history['val_loss']), 16):\n",
    "    history_mask.history['val_loss'].append(last_val_loss)\n",
    "    history_mask.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"mask\"] = history_mask.history['val_loss']\n",
    "augmentation_accuracy_df[f\"mask\"] = history_mask.history['val_accuracy']\n",
    "\n",
    "# Rand noise and zoom\n",
    "history_rand_noise_zoom = best_model_rand_noise_zoom.fit(train_generator_rand_noise_zoom,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_rand_noise_zoom.history['val_loss'][-1]\n",
    "last_val_accuracy = history_rand_noise_zoom.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_rand_noise_zoom.history['val_loss']), 16):\n",
    "    history_rand_noise_zoom.history['val_loss'].append(last_val_loss)\n",
    "    history_rand_noise_zoom.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"rand_noise_zoom\"] = history_rand_noise_zoom.history['val_loss']\n",
    "augmentation_accuracy_df[f\"rand_noise_zoom\"] = history_rand_noise_zoom.history['val_accuracy']\n",
    "\n",
    "# Random noise and mask\n",
    "history_rand_noise_mask = best_model_rand_noise_mask.fit(train_generator_rand_noise_mask,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_rand_noise_mask.history['val_loss'][-1]\n",
    "last_val_accuracy = history_rand_noise_mask.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_rand_noise_mask.history['val_loss']), 16):\n",
    "    history_rand_noise_mask.history['val_loss'].append(last_val_loss)\n",
    "    history_rand_noise_mask.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"rand_noise_mask\"] = history_rand_noise_mask.history['val_loss']\n",
    "augmentation_accuracy_df[f\"rand_noise_mask\"] = history_rand_noise_mask.history['val_accuracy']\n",
    "\n",
    "# No noise\n",
    "history_no_noise = best_model_no_noise.fit(train_generator_no_noise,\n",
    "                        epochs=15,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_no_noise.history['val_loss'][-1]\n",
    "last_val_accuracy = history_no_noise.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_no_noise.history['val_loss']), 16):\n",
    "    history_no_noise.history['val_loss'].append(last_val_loss)\n",
    "    history_no_noise.history['val_accuracy'].append(last_val_accuracy)\n",
    "augmentation_loss_df[f\"no_noise\"] = history_no_noise.history['val_loss']\n",
    "augmentation_accuracy_df[f\"no_noise\"] = history_no_noise.history['val_accuracy']\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "augmentation_loss_df.to_csv('augmentation_loss.csv', index=False)\n",
    "augmentation_accuracy_df.to_csv('augmentation_accuracy.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand noise test accuracy: 0.9676\n",
      "Rand noise test loss: 0.1507\n",
      "Radial noise test accuracy: 0.9590\n",
      "Radial noise test loss: 0.1452\n",
      "Flip test accuracy: 0.9489\n",
      "Flip test loss: 0.1888\n",
      "Zoom test accuracy: 0.9612\n",
      "Zoom test loss: 0.1526\n",
      "Mask test accuracy: 0.9679\n",
      "Mask test loss: 0.1315\n",
      "Rand noise and zoom test accuracy: 0.9663\n",
      "Rand noise and zoom test loss: 0.1416\n",
      "Rand noise and mask test accuracy: 0.9366\n",
      "Rand noise and mask test loss: 0.2489\n",
      "No noise test accuracy: 0.9481\n",
      "No noise test loss: 0.1800\n"
     ]
    }
   ],
   "source": [
    "rand_noise_loss, rand_noise_accuracy = best_model_rand_noise.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Rand noise test accuracy: {rand_noise_accuracy:.4f}\")\n",
    "print(f\"Rand noise test loss: {rand_noise_loss:.4f}\")\n",
    "radial_noise_loss, radial_noise_accuracy = best_model_radial_noise.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Radial noise test accuracy: {radial_noise_accuracy:.4f}\")\n",
    "print(f\"Radial noise test loss: {radial_noise_loss:.4f}\")\n",
    "flip_loss, flip_accuracy = best_model_flip.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Flip test accuracy: {flip_accuracy:.4f}\")\n",
    "print(f\"Flip test loss: {flip_loss:.4f}\")\n",
    "zoom_loss, zoom_accuracy = best_model_zoom.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Zoom test accuracy: {zoom_accuracy:.4f}\")\n",
    "print(f\"Zoom test loss: {zoom_loss:.4f}\")\n",
    "mask_loss, mask_accuracy = best_model_mask.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Mask test accuracy: {mask_accuracy:.4f}\")\n",
    "print(f\"Mask test loss: {mask_loss:.4f}\")\n",
    "rand_noise_zoom_loss, rand_noise_zoom_accuracy = best_model_rand_noise_zoom.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Rand noise and zoom test accuracy: {rand_noise_zoom_accuracy:.4f}\")\n",
    "print(f\"Rand noise and zoom test loss: {rand_noise_zoom_loss:.4f}\")\n",
    "rand_noise_mask_loss, rand_noise_mask_accuracy = best_model_rand_noise_mask.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Rand noise and mask test accuracy: {rand_noise_mask_accuracy:.4f}\")\n",
    "print(f\"Rand noise and mask test loss: {rand_noise_mask_loss:.4f}\")\n",
    "no_noise_loss, no_noise_accuracy = best_model_no_noise.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"No noise test accuracy: {no_noise_accuracy:.4f}\")\n",
    "print(f\"No noise test loss: {no_noise_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# List of models and their names\n",
    "models = [\n",
    "    (best_model_rand_noise, \"Random Noise\"),\n",
    "    (best_model_radial_noise, \"Radial Noise\"),\n",
    "    (best_model_flip, \"Flip\"),\n",
    "    (best_model_zoom, \"Zoom\"),\n",
    "    (best_model_mask, \"Mask\"),\n",
    "    (best_model_rand_noise_zoom, \"Random Noise + Zoom\"),\n",
    "    (best_model_rand_noise_mask, \"Random Noise + Mask\"),\n",
    "    (best_model_no_noise, \"No Noise\")\n",
    "]\n",
    "\n",
    "# Create and save confusion matrices for each model\n",
    "for model, name in models:\n",
    "    # Predictions on the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion = confusion_matrix(y_test, y_pred_class)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp.plot(ax=ax)\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    \n",
    "    # Save the figure\n",
    "    now = datetime.datetime.now()\n",
    "    plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\").lower()}_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.4245 - accuracy: 0.8686 - val_loss: 0.1133 - val_accuracy: 0.9658\n",
      "Epoch 2/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.2301 - accuracy: 0.9258 - val_loss: 0.0722 - val_accuracy: 0.9775\n",
      "Epoch 3/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1935 - accuracy: 0.9372 - val_loss: 0.0495 - val_accuracy: 0.9850\n",
      "Epoch 4/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1692 - accuracy: 0.9451 - val_loss: 0.0347 - val_accuracy: 0.9903\n",
      "Epoch 5/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1500 - accuracy: 0.9518 - val_loss: 0.0554 - val_accuracy: 0.9830\n",
      "Epoch 6/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.1354 - accuracy: 0.9563 - val_loss: 0.0339 - val_accuracy: 0.9903\n",
      "Epoch 7/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1296 - accuracy: 0.9583 - val_loss: 0.0342 - val_accuracy: 0.9893\n",
      "Epoch 8/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1239 - accuracy: 0.9600 - val_loss: 0.0415 - val_accuracy: 0.9873\n",
      "Epoch 9/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1180 - accuracy: 0.9619 - val_loss: 0.0337 - val_accuracy: 0.9897\n",
      "Epoch 10/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.1093 - accuracy: 0.9646 - val_loss: 0.0273 - val_accuracy: 0.9918\n",
      "Epoch 11/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.1066 - accuracy: 0.9657 - val_loss: 0.0330 - val_accuracy: 0.9890\n",
      "Epoch 12/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0999 - accuracy: 0.9680 - val_loss: 0.0292 - val_accuracy: 0.9918\n",
      "Epoch 13/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0983 - accuracy: 0.9681 - val_loss: 0.0271 - val_accuracy: 0.9912\n",
      "Epoch 14/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0938 - accuracy: 0.9695 - val_loss: 0.0246 - val_accuracy: 0.9922\n",
      "Epoch 15/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0948 - accuracy: 0.9688 - val_loss: 0.0271 - val_accuracy: 0.9923\n",
      "Epoch 16/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0892 - accuracy: 0.9709 - val_loss: 0.0243 - val_accuracy: 0.9925\n",
      "Epoch 17/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0892 - accuracy: 0.9714 - val_loss: 0.0290 - val_accuracy: 0.9910\n",
      "Epoch 18/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0846 - accuracy: 0.9726 - val_loss: 0.0282 - val_accuracy: 0.9923\n",
      "Epoch 19/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0827 - accuracy: 0.9731 - val_loss: 0.0235 - val_accuracy: 0.9935\n",
      "Epoch 20/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0831 - accuracy: 0.9735 - val_loss: 0.0252 - val_accuracy: 0.9922\n",
      "Epoch 21/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0792 - accuracy: 0.9744 - val_loss: 0.0257 - val_accuracy: 0.9927\n",
      "Epoch 22/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0827 - accuracy: 0.9729 - val_loss: 0.0258 - val_accuracy: 0.9918\n",
      "Epoch 23/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0778 - accuracy: 0.9745 - val_loss: 0.0242 - val_accuracy: 0.9932\n",
      "Epoch 24/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0718 - accuracy: 0.9766 - val_loss: 0.0232 - val_accuracy: 0.9928\n",
      "Epoch 25/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0744 - accuracy: 0.9762 - val_loss: 0.0279 - val_accuracy: 0.9920\n",
      "Epoch 26/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0725 - accuracy: 0.9759 - val_loss: 0.0254 - val_accuracy: 0.9925\n",
      "Epoch 27/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0715 - accuracy: 0.9766 - val_loss: 0.0251 - val_accuracy: 0.9938\n",
      "Epoch 28/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0705 - accuracy: 0.9775 - val_loss: 0.0304 - val_accuracy: 0.9927\n",
      "Epoch 29/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0654 - accuracy: 0.9786 - val_loss: 0.0227 - val_accuracy: 0.9937\n",
      "Epoch 30/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0708 - accuracy: 0.9766 - val_loss: 0.0217 - val_accuracy: 0.9938\n",
      "Epoch 31/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0651 - accuracy: 0.9781 - val_loss: 0.0250 - val_accuracy: 0.9932\n",
      "Epoch 32/50\n",
      "3375/3375 [==============================] - 16s 5ms/step - loss: 0.0678 - accuracy: 0.9773 - val_loss: 0.0220 - val_accuracy: 0.9937\n",
      "Epoch 33/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0654 - accuracy: 0.9790 - val_loss: 0.0222 - val_accuracy: 0.9952\n",
      "Epoch 34/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0645 - accuracy: 0.9785 - val_loss: 0.0265 - val_accuracy: 0.9932\n",
      "Epoch 35/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0619 - accuracy: 0.9802 - val_loss: 0.0269 - val_accuracy: 0.9932\n",
      "Epoch 36/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0621 - accuracy: 0.9795 - val_loss: 0.0242 - val_accuracy: 0.9932\n",
      "Epoch 37/50\n",
      "3375/3375 [==============================] - 16s 5ms/step - loss: 0.0622 - accuracy: 0.9794 - val_loss: 0.0223 - val_accuracy: 0.9937\n",
      "Epoch 38/50\n",
      "3375/3375 [==============================] - 16s 5ms/step - loss: 0.0627 - accuracy: 0.9793 - val_loss: 0.0254 - val_accuracy: 0.9923\n",
      "Epoch 39/50\n",
      "3375/3375 [==============================] - 16s 5ms/step - loss: 0.0626 - accuracy: 0.9793 - val_loss: 0.0248 - val_accuracy: 0.9930\n",
      "Epoch 40/50\n",
      "3375/3375 [==============================] - 16s 5ms/step - loss: 0.0582 - accuracy: 0.9808 - val_loss: 0.0235 - val_accuracy: 0.9932\n",
      "Epoch 1/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.2352 - accuracy: 0.9322 - val_loss: 0.0827 - val_accuracy: 0.9745\n",
      "Epoch 2/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0949 - accuracy: 0.9713 - val_loss: 0.1407 - val_accuracy: 0.9543\n",
      "Epoch 3/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0719 - accuracy: 0.9781 - val_loss: 0.0414 - val_accuracy: 0.9865\n",
      "Epoch 4/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0570 - accuracy: 0.9823 - val_loss: 0.0515 - val_accuracy: 0.9845\n",
      "Epoch 5/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0454 - accuracy: 0.9860 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
      "Epoch 6/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 0.0356 - val_accuracy: 0.9895\n",
      "Epoch 7/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.0377 - val_accuracy: 0.9890\n",
      "Epoch 8/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0313 - accuracy: 0.9900 - val_loss: 0.0362 - val_accuracy: 0.9903\n",
      "Epoch 9/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0273 - accuracy: 0.9909 - val_loss: 0.0420 - val_accuracy: 0.9887\n",
      "Epoch 10/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0270 - accuracy: 0.9911 - val_loss: 0.0325 - val_accuracy: 0.9908\n",
      "Epoch 11/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0229 - accuracy: 0.9924 - val_loss: 0.0330 - val_accuracy: 0.9900\n",
      "Epoch 12/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0296 - val_accuracy: 0.9912\n",
      "Epoch 13/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.0349 - val_accuracy: 0.9910\n",
      "Epoch 14/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0328 - val_accuracy: 0.9912\n",
      "Epoch 15/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.0336 - val_accuracy: 0.9908\n",
      "Epoch 16/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0170 - accuracy: 0.9939 - val_loss: 0.0263 - val_accuracy: 0.9918\n",
      "Epoch 17/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0437 - val_accuracy: 0.9918\n",
      "Epoch 18/50\n",
      "3375/3375 [==============================] - 17s 5ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0356 - val_accuracy: 0.9917\n",
      "Epoch 19/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0453 - val_accuracy: 0.9898\n",
      "Epoch 20/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.0346 - val_accuracy: 0.9910\n",
      "Epoch 21/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0384 - val_accuracy: 0.9907\n",
      "Epoch 22/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0361 - val_accuracy: 0.9918\n",
      "Epoch 23/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0306 - val_accuracy: 0.9928\n",
      "Epoch 24/50\n",
      "3375/3375 [==============================] - 19s 6ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0314 - val_accuracy: 0.9923\n",
      "Epoch 25/50\n",
      "3375/3375 [==============================] - 18s 5ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0353 - val_accuracy: 0.9922\n",
      "Epoch 26/50\n",
      "3375/3375 [==============================] - 19s 5ms/step - loss: 0.0119 - accuracy: 0.9958 - val_loss: 0.0290 - val_accuracy: 0.9933\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Mask Model Test Accuracy: 0.9780\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Random Noise Zoom Model Test Accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "best_model_mask_long = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "train_generator_mask_long = datagen_mask.flow(x_train, y_train, batch_size=16)\n",
    "history_mask_long = best_model_mask_long.fit(train_generator_mask_long,\n",
    "                        epochs=50,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_mask_long.history['val_loss'][-1]\n",
    "last_val_accuracy = history_mask_long.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_mask_long.history['val_loss']), 50):\n",
    "    history_mask_long.history['val_loss'].append(last_val_loss)\n",
    "    history_mask_long.history['val_accuracy'].append(last_val_accuracy)\n",
    "\n",
    "best_model_rand_noise_zoom_long = create_cnn_model(input_shape, num_classes, hyperparameters)\n",
    "train_generator_rand_noise_zoom_long = datagen_rand_noise_zoom.flow(x_train, y_train, batch_size=16)\n",
    "history_rand_noise_zoom_long = best_model_rand_noise_zoom_long.fit(train_generator_rand_noise_zoom_long,\n",
    "                        epochs=50,\n",
    "                        validation_data=val_generator,\n",
    "                        verbose=1,\n",
    "                        callbacks=[early_stopping]\n",
    "                        )\n",
    "last_val_loss = history_rand_noise_zoom_long.history['val_loss'][-1]\n",
    "last_val_accuracy = history_rand_noise_zoom_long.history['val_accuracy'][-1]\n",
    "for _ in range(len(history_rand_noise_zoom_long.history['val_loss']), 50):\n",
    "    history_rand_noise_zoom_long.history['val_loss'].append(last_val_loss)\n",
    "    history_rand_noise_zoom_long.history['val_accuracy'].append(last_val_accuracy)\n",
    "\n",
    "# Predictions on the test set for mask long model\n",
    "y_pred_mask = best_model_mask.predict(x_test)\n",
    "y_pred_mask_class = np.argmax(y_pred_mask, axis=1)\n",
    "# Calculate and print accuracy for mask model\n",
    "mask_accuracy = accuracy_score(y_test, y_pred_mask_class)\n",
    "print(f\"Mask Model Test Accuracy: {mask_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Confusion matrix for mask long\n",
    "confusion_mask = confusion_matrix(y_test, y_pred_mask_class)\n",
    "disp_mask = ConfusionMatrixDisplay(confusion_matrix=confusion_mask)\n",
    "\n",
    "# Plot mask long confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp_mask.plot(ax=ax)\n",
    "plt.title('Confusion Matrix for Mask Long')\n",
    "\n",
    "# Save the mask long figure\n",
    "now = datetime.datetime.now()\n",
    "plt.savefig(f'confusion_matrix_mask_long_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "plt.close()\n",
    "\n",
    "# Predictions on the test set for noise zoom long model \n",
    "y_pred_noise = best_model_rand_noise_zoom.predict(x_test)\n",
    "y_pred_noise_class = np.argmax(y_pred_noise, axis=1)\n",
    "# Calculate and print accuracy for random noise zoom model \n",
    "noise_accuracy = accuracy_score(y_test, y_pred_noise_class)\n",
    "print(f\"Random Noise Zoom Model Test Accuracy: {noise_accuracy:.4f}\")\n",
    "# Confusion matrix for noise zoom long\n",
    "confusion_noise = confusion_matrix(y_test, y_pred_noise_class)\n",
    "disp_noise = ConfusionMatrixDisplay(confusion_matrix=confusion_noise)\n",
    "\n",
    "# Plot noise zoom long confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp_noise.plot(ax=ax)\n",
    "plt.title('Confusion Matrix for Random Noise Zoom Long')\n",
    "\n",
    "# Save the noise zoom long figure\n",
    "plt.savefig(f'confusion_matrix_random_noise_zoom_long_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "plt.close()\n",
    "# Plot learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot training & validation loss values for mask model\n",
    "plt.plot(history_mask_long.history['val_accuracy'], label='Validation Accuracy Mask ')\n",
    "plt.plot(history_rand_noise_zoom_long.history['val_accuracy'], label='Validation Accuracy Random Noise Zoom')\n",
    "plt.title('Augmentation Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'learning_curves_{now.strftime(\"%Y-%m-%d\")}.png')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
